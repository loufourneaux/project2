# -*- coding: utf-8 -*-
"""neural_network_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c5t3YPZYrPCI7GyjgeO094ft8aa7EEbH
"""

import subprocess

# Installation on Google Colab
try:
    import os
    import google.colab
    subprocess.run(['python', '-m', 'pip', 'install', 'skorch', 'torchvision'])
    subprocess.run(['mkdir', '-p', 'datasets'])
    subprocess.run(['wget', '-nc', '--no-check-certificate', 'https://download.pytorch.org/tutorial/hymenoptera_data.zip', '-P', 'datasets'])
    subprocess.run(['unzip', '-u', 'datasets/hymenoptera_data.zip', '-d' 'datasets'])
except ImportError:
    pass

# @title
from google.colab import drive
drive.mount('/content/drive')

import torch.nn as nn
import torch.optim as optim
import torch
from skorch.callbacks import EarlyStopping
from skorch import NeuralNetClassifier
from sklearn.model_selection import GridSearchCV

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import random
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report

def data_preprocessing(model_name, task_name, batchsize, temp_size = 0.2, test_size =0.5, num_channels =3):
    """

    :param model_name:
    :param batchsize:
    :param temp_size:
    :param test_size:
    :param num_channels:
    :return:
    """
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    df = pd.read_parquet('/content/drive/My Drive/project2/All_Relative_Results_Cleaned.parquet')

    df_clean = df.dropna()
    index = df_clean.columns.get_loc('time(s)')

    if task_name == 'Task 1':
        Y = df_clean['Exercise']
    elif task_name == 'Task 2':
        #Fusion of the column exercise and Set to know the mistake
        Y = df_clean['Exercise']+ ' with mistake '+ df_clean['Set']
    label_encoder = LabelEncoder()
    Y_encoded = label_encoder.fit_transform(Y)


    if model_name == 'NN' or model_name == 'CNN':
        index +=1

    X = df_clean.iloc[:, index:]

    if model_name == 'NN':
        X_tensor = torch.tensor(X.values, dtype=torch.float32)

    elif model_name =='CNN':
        cols_x = [col for col in X.columns if col.endswith('x')]
        cols_y = [col for col in X.columns if col.endswith('y')]
        cols_z = [col for col in X.columns if col.endswith('z')]
        tensor_5D = torch.zeros((len(X), 3, 33, 1, 1), dtype=torch.float32)
        nbr_of_points = (X.shape[1])//3
        X_reshape = X.values.reshape(-1,nbr_of_points,3)
        tensor_4D_transposed = np.expand_dims(np.expand_dims(np.transpose(X_reshape, (0, 2, 1)), axis=3), axis=4)
        tensor_5D = torch.from_numpy(tensor_4D_transposed)
        tensor_5D = tensor_5D.permute(0, 2, 1, 3, 4)

        data_x = X[cols_x].values
        data_y = X[cols_y].values
        data_z = X[cols_z].values

        mean_x = np.mean(data_x, axis=0)
        mean_y = np.mean(data_y, axis=0)
        mean_z = np.mean(data_z, axis=0)

        mean_coords = np.stack([mean_x, mean_y, mean_z], axis=1)
        pca = PCA(n_components=3)
        reduced_coords = pca.fit_transform(mean_coords)


        normalized_coords = (reduced_coords - reduced_coords.min(0)) / reduced_coords.ptp(0)
        print(normalized_coords.shape)
        grid_coords = np.round(normalized_coords * np.array([2, 2, 10])).astype(int)
        depth, height, width = 3, 3, 11

        num_channels = num_channels

        num_samples = 2183485
        grid_tensor = torch.zeros((num_samples, num_channels, depth, height, width))
        for feature_idx in range(33):  # Supposons que vous avez 33 features
            d, h, w = grid_coords[feature_idx]

            d, h, w = np.clip([d, h, w], 0, [depth - 1, height - 1, width - 1])

            grid_tensor[:, 0, d, h, w] = torch.tensor(X[cols_x[feature_idx]].values)
            grid_tensor[:, 1, d, h, w] = torch.tensor(X[cols_y[feature_idx]].values)
            grid_tensor[:, 2, d, h, w] = torch.tensor(X[cols_z[feature_idx]].values)

            X_tensor = grid_tensor

    Y_tensor = torch.tensor(Y_encoded, dtype=torch.long)

    X_train, X_temp, Y_train, Y_temp = train_test_split(X_tensor, Y_tensor, test_size=temp_size)
    X_test, X_validation, Y_test, Y_validation = train_test_split(X_temp, Y_temp, test_size=test_size)


    train_dataset = TensorDataset(X_train,Y_train)
    #validation_dataset = TensorDataset(X_validation,Y_validation)
    test_dataset = TensorDataset(X_test,Y_test)
    trainLoader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)
    #validationLoader = DataLoader(validation_dataset,batch_size=batchsize,shuffle=True)
    testLoader = DataLoader(test_dataset, batch_size=batchsize , shuffle=True)

    return trainLoader, testLoader, X_validation, Y_validation

class NeuralNetwork(nn.Module):
    def __init__(self,  output_size, input_size=99, hidden_size1=64,hidden_size2=64):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size1)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size1, hidden_size2)
        self.layer3=nn.Linear(hidden_size2, output_size)

        self.reset_parameters()

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x= self.relu(x)
        x=self.layer3(x)
        return x

    def reset_parameters(self):
        for layer in self.children():
            if hasattr(layer, 'reset_parameters'):
                layer.reset_parameters()

model = NeuralNetwork()

def restart_nn_model(learning_rate = 0.001):
    """

    :param learning_rate:
    :return:
    """

    model = NeuralNetwork()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

def get_nn_model():
    """

    :return:
    """
    return model

def train_nn_model(trainLoader, model, learning_rate = 0.001, nbr_epoch = 100):
    """

    :param trainLoader:
    :param learning_rate:
    :param number_epoch:
    :return:
    """
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    model.train()

    for epoch in range(nbr_epoch):
        running_loss = 0.0
        total_train = 0
        correct_train = 0

        for inputs, labels in trainLoader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(trainLoader)
        epoch_accuracy = 100 * correct_train / total_train
        print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')
    return model

def test_nn_model(testLoader, model):
    """

    :param testLoader:
    :param model:
    :return:
    """
    model.eval()
    total_test = 0
    correct_test = 0

    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, labels in testLoader:
            labels = labels.long()
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())
            total_test += labels.size(0)
            correct_test += (predicted == labels).sum().item()

    test_accuracy = 100 * correct_test / total_test
    print(f'Accuracy on test set: {test_accuracy}%')
    print(classification_report(y_true, y_pred))

def tune_nn_hyperparameters(X_valid, y_valid, outputsize):
    #parameters to tune
    param_grid = {
        'module__hidden_size1': [4096,2048],
        'module__hidden_size2': [512],
        'batch_size': [153, 150, 152, 151],
        'optimizer__lr': [0.001]
    }



    early_stopping = EarlyStopping(
        monitor='valid_loss',  # Change to 'valid_acc' for accuracy
        threshold=0.0001,       # Define your threshold
        threshold_mode='rel',  # 'rel' for relative, 'abs' for absolute
        patience=5            # Number of epochs to wait after condition is met
    )


    # Convert the PyTorch model to a skorch classifier to use in GridSearchCV
    classifier = NeuralNetClassifier(
        model,
        criterion=nn.CrossEntropyLoss,
        optimizer=optim.Adam,
        max_epochs=50, # or choose an appropriate number of epochs
        callbacks=[early_stopping],
        module__output_size=outputsize
    )

    # Use GridSearchCV for hyperparameter tuning, cv for the number of folds in cross-validation, verbose for the explicit stage of tuning
    grid_search = GridSearchCV(classifier, param_grid, scoring='accuracy', cv=3, verbose=1)
    #get grid result
    grid_result= grid_search.fit(X_valid, y_valid)

    # Get the best hyperparameters
    best_hyperparams = grid_search.best_params_


    #get best score
    best_score=grid_search.best_score_

    return best_hyperparams, best_score

"""Run file for task1"""

#data preprocessing
trainLoader,testLoader, X_valid, Y_valid = data_preprocessing('NN', 'Task 1', 151)

#initialize model with defauflt parameters
model = NeuralNetwork(output_size=7)
#train the model
model = train_nn_model(trainLoader, model, nbr_epoch=1)

#test model
print("TESTING")
test_nn_model(testLoader, model)

#tune hyperparameters
print("TUNING")
best_hyperparams, best_score = tune_nn_hyperparameters(X_valid, Y_valid, outputsize=7)

#CREATE A BEST MODEL
best_model = NeuralNetwork(hidden_size1 = best_hyperparams.hidden_size1, hidden_size2 = best_hyperparams.hidden_size2, output_size=7)
#train the best model
best_model = train_nn_model(trainLoader, best_model, nbr_epoch=50)

#torch.save(best_model, 'path/to/save/entire_model.pth')
print("TESTING TUNED MODEL")
test_nn_model(testLoader, best_model)

#data preprocessing
trainLoader,testLoader, X_valid, Y_valid = data_preprocessing('NN', 'Task 2', 151)

#initialize model with defauflt parameters  and change output size
model = NeuralNetwork(output_size=38)

#train the model
model=train_nn_model(trainLoader,model, nbr_epoch=1)

#test model on a different subset of data
print("TESTING")
test_nn_model(testLoader, model)

#tune hyperparameters of the model
print("TUNING")
best_hyperparams, best_score = tune_nn_hyperparameters(X_valid, Y_valid, outputsize=38)

#test model again with hyperparameters
best_model = NeuralNetwork(hidden_size1 = best_hyperparams.hidden_size1, hidden_size2 = best_hyperparams.hidden_size2, output_size=38)

#train best model
best_model = train_nn_model(trainLoader, best_model)
#torch.save(best_model, 'path/to/save/entire_model.pth')
print("TESTING TUNED MODEL")
test_nn_model(testLoader, best_model)

